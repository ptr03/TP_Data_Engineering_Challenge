# TP Data Engineering Challenge ‚Äî Phase 1 + Phase 2

Dieses Repository enth√§lt die vollst√§ndige Abgabe f√ºr die Touchpoint Case Study:

- **Phase 1:** CSV-Validierung & Import in Supabase (Schema + Import-Script).
- **Phase 2:** Frontend Dashboard (React + Vite) mit KPIs, Campaign Table, Spend- und ROAS-Charts.

## Deliverables

- `frontend/` ‚Äî React + Vite App (Dashboard: KPIs, Tabelle, Charts)
- `campaign_schema.sql` ‚Äî SQL zur Erstellung der Tabellen in Supabase
- `csv_validator.js` ‚Äî CSV-Validator (Format- + Logikchecks)
- `import.js` ‚Äî Batch Importer nach Supabase (nutzt Service Role Key)
- `data/campaigns.csv` ‚Äî Rohdaten (CSV)
- `env.example` ‚Äî Vorlage f√ºr Umgebungsvariablen (Server & Frontend)
- `Ergebnisse/` ‚Äî Screenshots / CSV-Exports der SQL-Checks

Quickstart

### 1. Node.js installieren

Installiere die aktuelle **LTS-Version** von Node.js, falls noch nicht vorhanden.

---

### 2. Repository klonen

Klon das Projekt und √∂ffne anschlie√üend den Projektordner im Root-Verzeichnis.

---

### 3. Root-Dependencies installieren

```bash
npm install
```

---

### 4. Umgebungsvariablen (Server-Seite) einrichten

Erstelle im Root-Verzeichnis eine `.env`-Datei auf Basis des Beispiels:

```bash
cp env.example .env
```

F√ºlle anschlie√üend in der `.env` die Werte f√ºr:

```
SUPABASE_URL
SUPABASE_SERVICE_KEY
```

---

### 5. Frontend-Umgebung konfigurieren

Wechsle in den `frontend`-Ordner und lege dort eine `.env.local`-Datei an.
Kopiere die Variablen aus `env.example` und setze die Werte f√ºr:

```
VITE_SUPABASE_URL
VITE_SUPABASE_ANON_KEY
```

---

### 6. Daten pr√ºfen & importieren

Zur Validierung und zum Import der Daten (z. B. Kampagnendaten):

```bash
npm run validate
node import.js data/campaigns.csv
```

---

### 7. Frontend starten

```bash
cd frontend
npm install
npm run dev
```

Danach kannst du das Frontend unter
üëâ **[http://localhost:5173](http://localhost:5173)**
im Browser √∂ffnen.

---

### 8. (Optional) Production Build erstellen

Falls du das Projekt f√ºr den Live-Betrieb bauen m√∂chtest:

```bash
npm run build
```

(diesen Befehl im `frontend`-Ordner ausf√ºhren)

## Design-Entscheidungen

- **Schema:** normalisiert in `campaigns` (Stammdaten) und `campaign_metrics` (time series). Vorteil: saubere Aggregationen, Indexierung und verhindert Redundanz.
- **Datentypen:** `bigint` f√ºr Counts, `numeric(14,2)` f√ºr Geldwerte ‚Üí pr√§zise Summen/ROAS.
- **Import:** pre-validation ‚Üí batch processing ‚Üí upsert f√ºr campaigns, insert f√ºr metrics ‚Üí errors nach `errors/`.
- **Security:** Service Role Key wird **nicht** ins Repo gepusht. `.env` ist in `.gitignore`.

## Wie die Daten importiert wurden

1. CSV aus Google Sheets exportiert (UTF-8, comma).
2. `csv_validator.js` gepr√ºft alles auf Format & Logik.
3. `campaign_schema.sql` in Supabase erstellt.
4. `import.js` ausgef√ºhrt (batch upserts/inserts).
5. SQL-Checks ausgef√ºhrt (counts, nulls, duplicates, logical checks) ‚Äî Ergebnisse in `evidence/`.

## Wichtige SQL-Checks (zum Kopieren ‚Üí Supabase SQL Editor)

```sql
-- Counts
SELECT 'campaigns' t, COUNT(*) FROM public.campaigns
UNION ALL
SELECT 'campaign_metrics', COUNT(*) FROM public.campaign_metrics;

-- Datumsspanne
SELECT MIN(date), MAX(date) FROM public.campaign_metrics;

-- Null/Negative checks
SELECT
 SUM(CASE WHEN impressions < 0 THEN 1 ELSE 0 END) AS neg_impr,
 SUM(CASE WHEN cost < 0 THEN 1 ELSE 0 END) AS neg_cost
FROM public.campaign_metrics;

-- Duplikate
SELECT date, campaign_id, COUNT(*) FROM public.campaign_metrics GROUP BY date, campaign_id HAVING COUNT(*) > 1;
```

## Herausforderungen & L√∂sungen

- **Datum / Dezimalformate:** CSV aus Sheets exportiert; Validator akzeptiert europ√§ische Dezimaltrennung.
- **Fehlende DB-Tabellen:** `campaign_schema.sql` zuerst ausf√ºhren.
- **Local dev access:** VPN/Proxy kann `localhost` blockieren (z.B. NordVPN) ‚Äî Dev-Server nutzbar unter `http://127.0.0.1:5173/`.

## Was ich bei mehr Zeit machen w√ºrde

- Frontend: bessere UI/UX.
- Tests: automatisierte SQL tests + unit tests f√ºr Import/Validator.
- Production import: COPY from storage bucket
- Automatischer Datenfluss: CSV-Upload (lokal oder via Storage) triggert Import nach Supabase und aktualisiert das Frontend selbst√§ndig.

## Hinweise f√ºr Reviewer

- `.env` enth√§lt Secrets und ist in `.gitignore`. Niemals pushen.
- Falls ein Secret versehentlich gepusht wurde: sofort Key rotieren in Supabase.
- F√ºr das Frontend lege `frontend/.env.local` an und setze dort `VITE_SUPABASE_URL` und `VITE_SUPABASE_ANON_KEY`. Die Root-`.env` darf nur den Service-Key f√ºr `import.js` enthalten (niemals public).
- Security-Reminder: Wenn ein Service-Role-Key versehentlich gepusht wurde ‚Äî Key sofort in Supabase rotieren und ggf. Git-History s√§ubern.
